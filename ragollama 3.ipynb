{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc60dc8",
   "metadata": {},
   "source": [
    "## üß† What Is RAG and Why Should You Care?\n",
    "\n",
    "**RAG (Retrieval Augmented Generation)** is one of the most powerful techniques in modern AI applications. Let's break it down:\n",
    "\n",
    "| Component | What It Does | Why It Matters |\n",
    "|-----------|--------------|----------------|\n",
    "| **Retrieval** | Finds relevant information from your documents | Ensures answers come from *your* data, not just the AI's training |\n",
    "| **Augmentation** | Enhances the AI's knowledge with this specific information | Makes responses accurate and up-to-date |\n",
    "| **Generation** | Creates human-like responses using the retrieved information | Delivers insights in natural, easy-to-understand language |\n",
    "\n",
    "<div style=\"background-color: #effaf5; border: 1px solid #0d9488; padding: 15px; margin: 20px 0; border-radius: 5px;\">\n",
    "<h4 style=\"color: #000000; margin-top: 0;\">üí° Real-World Analogy</h4>\n",
    "<p style=\"color: #000000;\">Think of RAG as the difference between:</p>\n",
    "<ul style=\"color: #000000;\">\n",
    "<li><strong>A general knowledge expert</strong> who studied years ago (standard LLM)</li>\n",
    "<li><strong>A specialist with your documents open</strong> in front of them, referencing exact paragraphs as they answer your questions (RAG system)</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "## üõ†Ô∏è Our Exciting Toolkit\n",
    "\n",
    "We'll be using several cutting-edge tools to build our RAG system:\n",
    "\n",
    "| Tool | What It Is | Why It's Amazing |\n",
    "|------|------------|------------------|\n",
    "| **Ollama** | An open-source platform that runs AI models locally on your computer | Privacy (your data never leaves your machine), no API costs, and complete control |\n",
    "| **ChromaDB** | A specialized database for storing and searching \"vector embeddings\" | Lightning-fast semantic search that understands meaning, not just keywords |\n",
    "| **LangChain** | A framework that connects AI components together like building blocks | Makes complex AI workflows simple and customizable |\n",
    "| **Gradio** | A tool for creating web interfaces for AI models | Turns your code into a professional-looking application in minutes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16787e5d",
   "metadata": {},
   "source": [
    "# üéØ What We'll Build Together\n",
    "\n",
    "By the end of this tutorial, you'll have created:\n",
    "\n",
    "```\n",
    "üìÑ Documents ‚Üí üî™ Chunker ‚Üí üßÆ Vector DB ‚Üí üîç Retriever ‚Üí ü§ñ LLM ‚Üí üí¨ Answer\n",
    "```\n",
    "\n",
    "A complete RAG system that can:\n",
    "\n",
    "1. **Process PDF documents** of your choice\n",
    "2. **Break them into smart chunks** that preserve meaning\n",
    "3. **Transform text into vectors** that capture semantic meaning\n",
    "4. **Store everything efficiently** for lightning-fast retrieval\n",
    "5. **Find the most relevant information** for any question\n",
    "6. **Generate accurate, helpful responses** with proper citations\n",
    "\n",
    "<div style=\"background-color: #ffe4e6; border-left: 6px solid #be123c; padding: 15px; margin: 20px 0; border-radius: 5px;\">\n",
    "<h3 style=\"color: #000000; margin-top: 0;\">üî• Why This Matters For Your Career</h3>\n",
    "<p style=\"color: #000000;\">RAG systems are at the forefront of practical AI applications. At MAIA Academy, we've seen how companies are rapidly adopting this technology to:</p>\n",
    "<ul style=\"color: #000000;\">\n",
    "<li>Build intelligent document assistants</li>\n",
    "<li>Create knowledge bases that actually answer questions</li>\n",
    "<li>Develop customer support systems that handle complex queries</li>\n",
    "<li>Implement research tools that synthesize information from multiple sources</li>\n",
    "</ul>\n",
    "<p style=\"color: #000000;\">The skills you'll learn today are directly transferable to real-world AI projects and align perfectly with our <strong>Foundations of AI Development</strong> and <strong>Deep Learning & LLMs</strong> modules!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91424061",
   "metadata": {},
   "source": [
    "## 2. Setting the Stage: Configuration\n",
    "\n",
    "<div style=\"background-color: #2d333b; padding: 20px; border-radius: 8px; margin-bottom: 20px; border-left: 6px solid #58a6ff;\">\n",
    "  <h3 style=\"color: #ffffff; margin-top: 0;\">System Configuration Parameters</h3>\n",
    "  <p style=\"color: #ffffff;\">Before we build our RAG system, we need to configure some important settings‚Äîlike tuning a new instrument before a performance. These parameters will determine how our system processes and interacts with documents.</p>\n",
    "</div>\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse; margin: 20px 0; background-color: #22272e;\">\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; width: 200px;\"><strong style=\"color: #58a6ff;\">PERSIST_DIRECTORY</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">Where we'll store our \"data safe\" (the vector database) on disk. This allows our system to remember what it learned even after restarting.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">CHUNK_SIZE</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">How big each text piece will be (in characters). This affects how much context the AI has when answering questions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">CHUNK_OVERLAP</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">How much the pieces overlap to maintain context between chunks and ensure no information is lost at the boundaries.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">PDF_URLS</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">The documents we'll use as our knowledge base (our \"reference library\"). These are the sources the system will learn from.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">LLM_MODEL</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">The \"brain\" that processes the context and generates answers (like llama3 or other models available in Ollama).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">EMBEDDING_MODEL</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">The \"translator\" that converts text into numerical vectors that capture meaning. Different models balance between speed and accuracy.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border: 1px solid #444c56; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h3 style=\"color: #58a6ff; margin-top: 0;\">üí° What's This Chunk Stuff?</h3>\n",
    "  <p style=\"color: #adbac7;\">Think of cutting a big sandwich. If the pieces are huge, you get more filling but it's hard to bite. If they're tiny, you bite easy but might miss the full flavor. Overlap is like leaving a bit of the last bite on the next one so you don't lose track of the overall taste.</p>\n",
    "  \n",
    "  <div style=\"display: flex; justify-content: space-between; margin-top: 20px; text-align: center;\">\n",
    "    <div style=\"flex: 1; margin: 0 10px;\">\n",
    "      <p style=\"color: #58a6ff;\"><strong>Large Chunks (2000+)</strong></p>\n",
    "      <p style=\"color: #adbac7;\">‚úÖ More context<br>‚úÖ Better for complex topics<br>‚ùå Less precise retrieval<br>‚ùå Slower processing</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin: 0 10px;\">\n",
    "      <p style=\"color: #58a6ff;\"><strong>Medium Chunks (800-1200)</strong></p>\n",
    "      <p style=\"color: #adbac7;\">‚úÖ Balanced approach<br>‚úÖ Good for most cases<br>‚úÖ Reasonable speed<br>‚úÖ Decent precision</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin: 0 10px;\">\n",
    "      <p style=\"color: #58a6ff;\"><strong>Small Chunks (300-500)</strong></p>\n",
    "      <p style=\"color: #adbac7;\">‚úÖ Very precise retrieval<br>‚úÖ Fast processing<br>‚ùå Limited context<br>‚ùå May miss broader concepts</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; background-color: #22272e; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #ff7b72; font-weight: bold;\">‚ö†Ô∏è Warning</p>\n",
    "  <p style=\"color: #adbac7;\">This notebook is designed to be read with a dark background. If you program with a white background, just know that you're a complete psychopath and a danger to society.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install libraries (only the first time): !pip install ollama chroma langchain gradio langchain_ollama langchain_community pypdf\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Gradio for web interface\n",
    "import gradio as gr\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c106191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "from tkinter.tix import MAX\n",
    "\n",
    "\n",
    "PERSIST_DIRECTORY = 'chroma_db' # directory to store the vector database\n",
    "CHUNK_SIZE = 1000 # characters per chunk for text splitting\n",
    "CHUNK_OVERLAP = 50 # characters of overlap between chunks\n",
    "PDF_URLS = [os.path.join('data', f) for f in os.listdir('data') if f.endswith('.pdf')]\n",
    "LLM_MODEL = 'qwen3:1.7b'\n",
    "EMBEDDING_MODEL = 'all-minilm:latest'\n",
    "TEMPERATURE = 0.7  # Increased temperature for more natural variation in responses\n",
    "MAX_TOKENS = 1024  # Control response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ac3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, pdf_urls: List[str], persist_directory: str = PERSIST_DIRECTORY):\n",
    "        self.pdf_urls = pdf_urls\n",
    "        self.persist_directory = persist_directory\n",
    "        self.documents = []\n",
    "        self.vectorstore = None\n",
    "        self.llm = None\n",
    "        self.chain = None\n",
    "        \n",
    "        # Initialize the LLM with streaming capability and additional parameters\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = ChatOllama(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            callback_manager=callback_manager,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            top_p=0.9,      # Nucleus sampling\n",
    "            top_k=40,       # Top-k sampling\n",
    "            repeat_penalty=1.2  # Penalize repetition\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "        \n",
    "        logger.info(f\"Initialized RAG system with {len(pdf_urls)} PDFs\")\n",
    "\n",
    "    def load_documents(self) -> None:\n",
    "        \"\"\"Load and split PDF documents\"\"\"\n",
    "        logger.info(\"Loading and processing PDFs...\")\n",
    "        logger.info(f\"Attempting to load {len(self.pdf_urls)} PDFs\")\n",
    "        \n",
    "        # Text splitter for chunking documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        all_pages = []\n",
    "        successful_loads = 0\n",
    "        \n",
    "        for url in self.pdf_urls:\n",
    "            try:\n",
    "                logger.info(f\"Attempting to load: {url}\")\n",
    "                loader = PyPDFLoader(url)\n",
    "                pages = loader.load()\n",
    "                logger.info(f\"Successfully loaded {len(pages)} pages from {url}\")\n",
    "                all_pages.extend(pages)\n",
    "                successful_loads += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading PDF from {url}: {str(e)}\")\n",
    "                logger.error(f\"Exception type: {type(e).__name__}\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {successful_loads} out of {len(self.pdf_urls)} PDFs\")\n",
    "        \n",
    "        # Split the documents into chunks\n",
    "        self.documents = text_splitter.split_documents(all_pages)\n",
    "        logger.info(f\"Created {len(self.documents)} document chunks\")\n",
    "\n",
    "    def create_vectorstore(self) -> None:\n",
    "        \"\"\"Create a fresh vector database\"\"\"\n",
    "        # Remove any existing database\n",
    "        if os.path.exists(self.persist_directory):\n",
    "            import shutil\n",
    "            logger.info(f\"Removing existing vectorstore at {self.persist_directory}\")\n",
    "            shutil.rmtree(self.persist_directory, ignore_errors=True)\n",
    "        \n",
    "        # Create a new vector store\n",
    "        logger.info(\"Creating new vectorstore...\")\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.persist_directory\n",
    "        )\n",
    "        logger.info(\"Vectorstore creation complete\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Enhanced text cleaning function to remove various types of repetitions.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Function to remove consecutive duplicate words\n",
    "        def remove_word_repetitions(text):\n",
    "            # Split text into words while preserving punctuation and spacing\n",
    "            tokens = []\n",
    "            current_word = []\n",
    "            \n",
    "            for char in text:\n",
    "                if char.isalnum() or char in \"-\":\n",
    "                    current_word.append(char)\n",
    "                else:\n",
    "                    if current_word:\n",
    "                        tokens.append(''.join(current_word))\n",
    "                        current_word = []\n",
    "                    tokens.append(char)\n",
    "                    \n",
    "            if current_word:\n",
    "                tokens.append(''.join(current_word))\n",
    "            \n",
    "            # Remove consecutive duplicates (case-insensitive)\n",
    "            cleaned = []\n",
    "            for i, token in enumerate(tokens):\n",
    "                if i > 0:\n",
    "                    prev_token = cleaned[-1]\n",
    "                    # Skip if it's the same word (case-insensitive)\n",
    "                    if (token.strip().lower() == prev_token.strip().lower() and \n",
    "                        token.strip() and prev_token.strip() and \n",
    "                        token.strip().isalnum()):\n",
    "                        continue\n",
    "                cleaned.append(token)\n",
    "            \n",
    "            return ''.join(cleaned)\n",
    "        \n",
    "        # Function to fix spacing issues\n",
    "        def fix_spacing(text):\n",
    "            # Fix multiple spaces\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Fix spaces around punctuation\n",
    "            text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "            # Ensure single space after punctuation\n",
    "            text = re.sub(r'([.,!?;:])(?!\\s)', r'\\1 ', text)\n",
    "            return text.strip()\n",
    "        \n",
    "        # Function to remove repeated phrases\n",
    "        def remove_phrase_repetitions(text):\n",
    "            # Remove repeated phrases of 2-4 words\n",
    "            for phrase_length in range(2, 5):\n",
    "                pattern = r'\\b(\\w+(?:\\s+\\w+){' + str(phrase_length-1) + r'})\\s+\\1\\b'\n",
    "                text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n",
    "            return text\n",
    "        \n",
    "        # Apply all cleaning steps\n",
    "        text = remove_word_repetitions(text)\n",
    "        text = remove_phrase_repetitions(text)\n",
    "        text = fix_spacing(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def setup_chain(self) -> None:\n",
    "        \"\"\"Set up the RAG chain with improved response processing\"\"\"\n",
    "        # Create the retriever\n",
    "        if not self.vectorstore:\n",
    "            self.create_vectorstore()\n",
    "            \n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a robotics and wearable robots expert. Answer questions about physical dummies in human-exoskeleton interaction research based strictly on the provided documents.\n",
    "\n",
    "Context from papers:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "1. Provide clear, direct answers using technical terminology from the papers\n",
    "2. Use citations [Author Year] for key points\n",
    "3. Focus on accuracy and clarity\n",
    "4. Use natural, varied language - avoid repetition\n",
    "5. If information isn't in the context, say so directly\n",
    "6. End with referenced sources\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        \n",
    "        # Create the chain with enhanced text processing\n",
    "        self.chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "            | self.clean_text  # Apply enhanced text cleaning\n",
    "        )\n",
    "        \n",
    "        logger.info(\"RAG chain setup complete\")\n",
    "\n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question using the RAG chain\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            \n",
    "        Returns:\n",
    "            The answer to the question\n",
    "        \"\"\"\n",
    "        if not self.chain:\n",
    "            self.setup_chain()\n",
    "        \n",
    "        logger.info(f\"Answering question: {question}\")\n",
    "        try:\n",
    "            answer = self.chain.invoke(question)\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error answering question: {e}\")\n",
    "            return f\"Error processing your question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d100c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(self) -> None:\n",
    "    \"\"\"Load and split PDF documents\"\"\"\n",
    "    logger.info(\"Loading and processing PDFs...\")\n",
    "    logger.info(f\"Attempting to load {len(self.pdf_urls)} PDFs\")\n",
    "    \n",
    "    # Text splitter for chunking documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_pages = []\n",
    "    successful_loads = 0\n",
    "    \n",
    "    for url in self.pdf_urls:\n",
    "        try:\n",
    "            logger.info(f\"Attempting to load: {url}\")\n",
    "            loader = PyPDFLoader(url)\n",
    "            pages = loader.load()\n",
    "            logger.info(f\"Successfully loaded {len(pages)} pages from {url}\")\n",
    "            all_pages.extend(pages)\n",
    "            successful_loads += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF from {url}: {str(e)}\")\n",
    "            logger.error(f\"Exception type: {type(e).__name__}\")\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {successful_loads} out of {len(self.pdf_urls)} PDFs\")\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    self.documents = text_splitter.split_documents(all_pages)\n",
    "    logger.info(f\"Created {len(self.documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e4af3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2d333b; padding: 5px; border-radius: 4px; margin-bottom: 10px;\">\n",
    "  <h3 style=\"color: #58a6ff; margin: 10px;\">3.2 Storing in a Vector Database</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 15px; border-radius: 8px; border-left: 4px solid #f0883e; margin-bottom: 20px;\">\n",
    "  <p style=\"color: #adbac7;\">After chunking our documents, we need to store them in a way that allows for intelligent searching. This is where vectors and ChromaDB come into play.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border: 1px solid #444c56; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h4 style=\"color: #f0883e; margin-top: 0;\">üßÆ What Are Vectors?</h4>\n",
    "  \n",
    "  <p style=\"color: #adbac7;\">Think of each chunk as a person, and we give it a unique \"fingerprint\" based on what it says. These fingerprints are actually lists of numbers that capture meaning.</p>\n",
    "  \n",
    "  <div style=\"display: flex; margin-top: 20px; background-color: #22272e; padding: 15px; border-radius: 8px;\">\n",
    "    <div style=\"flex: 1; padding-right: 15px;\">\n",
    "      <p style=\"color: #adbac7; font-style: italic; margin-top: 0;\">\"I like the sun\"</p>\n",
    "      <p style=\"color: #d2a8ff; font-family: monospace; font-size: 0.9em;\">[0.12, -0.33, 0.65, ...]</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; padding-left: 15px; border-left: 1px dashed #444c56;\">\n",
    "      <p style=\"color: #adbac7; font-style: italic; margin-top: 0;\">\"I love the heat\"</p>\n",
    "      <p style=\"color: #d2a8ff; font-family: monospace; font-size: 0.9em;\">[0.15, -0.28, 0.61, ...]</p>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <p style=\"color: #adbac7; margin-top: 20px;\">These sentences get similar vector \"fingerprints\" because they express similar concepts. This lets us search by <strong>meaning</strong>, not just exact words.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; background-color: #22272e; border-radius: 8px; overflow: hidden; margin: 20px 0; border: 1px solid #444c56;\">\n",
    "  <div style=\"flex: 1; padding: 15px; display: flex; flex-direction: column; align-items: center; text-align: center; border-right: 1px solid #444c56;\">\n",
    "    <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;\">\n",
    "      <span style=\"color: #58a6ff; font-weight: bold; font-size: 1.5em;\">1</span>\n",
    "    </div>\n",
    "    <p style=\"color: #f0883e; font-weight: bold; margin: 5px 0;\">Convert</p>\n",
    "    <p style=\"color: #adbac7; margin: 5px 0;\">Text ‚Üí Vector</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 15px; display: flex; flex-direction: column; align-items: center; text-align: center; border-right: 1px solid #444c56;\">\n",
    "    <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;\">\n",
    "      <span style=\"color: #58a6ff; font-weight: bold; font-size: 1.5em;\">2</span>\n",
    "    </div>\n",
    "    <p style=\"color: #f0883e; font-weight: bold; margin: 5px 0;\">Store</p>\n",
    "    <p style=\"color: #adbac7; margin: 5px 0;\">In ChromaDB</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 15px; display: flex; flex-direction: column; align-items: center; text-align: center;\">\n",
    "    <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;\">\n",
    "      <span style=\"color: #58a6ff; font-weight: bold; font-size: 1.5em;\">3</span>\n",
    "    </div>\n",
    "    <p style=\"color: #f0883e; font-weight: bold; margin: 5px 0;\">Retrieve</p>\n",
    "    <p style=\"color: #adbac7; margin: 5px 0;\">By Similarity</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 20px; border-radius: 8px; margin: 20px 0; border: 1px solid #444c56;\">\n",
    "  <h4 style=\"color: #58a6ff; margin-top: 0;\">In Plain English:</h4>\n",
    "  \n",
    "  <p style=\"color: #adbac7;\">1. <strong>We transform text into numbers</strong> using the embedding model (all-minilm)</p>\n",
    "  <p style=\"color: #adbac7;\">2. <strong>We store these numbers in ChromaDB</strong> along with the original text</p>\n",
    "  <p style=\"color: #adbac7;\">3. <strong>When you ask a question</strong>, we convert your question to a vector too</p>\n",
    "  <p style=\"color: #adbac7;\">4. <strong>ChromaDB finds chunks with similar vectors</strong> to your question</p>\n",
    "  <p style=\"color: #adbac7;\">5. <strong>These similar chunks</strong> likely contain the answer you need</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; background-color: #22272e; border-radius: 8px; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; padding: 20px;\">\n",
    "    <h5 style=\"color: #7ee787; margin-top: 0;\">üí° Why This Is Cool</h5>\n",
    "    <ul style=\"color: #adbac7; list-style-type: none; padding-left: 0;\">\n",
    "      <li style=\"margin-bottom: 8px;\">‚úÖ <strong>Finds similar concepts</strong>, even with different words</li>\n",
    "      <li style=\"margin-bottom: 8px;\">‚úÖ <strong>Lightning-fast search</strong> of large document collections</li>\n",
    "      <li style=\"margin-bottom: 8px;\">‚úÖ <strong>Works across languages</strong> (Spanish \"sol\" ‚âà English \"sun\")</li>\n",
    "      <li>‚úÖ <strong>More accurate</strong> than keyword searching</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border-left: 4px solid #d2a8ff; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #adbac7; margin: 0;\"><strong style=\"color: #d2a8ff;\">üöÄ Pro Tip:</strong> Think of it like searching a music library - you find songs that \"sound similar\" to the one you like, not just songs with the exact same title.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48963ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(self) -> None:\n",
    "    \"\"\"Create a fresh vector database\"\"\"\n",
    "    # Remove any existing database\n",
    "    if os.path.exists(self.persist_directory):\n",
    "        import shutil\n",
    "        logger.info(f\"Removing existing vectorstore at {self.persist_directory}\")\n",
    "        shutil.rmtree(self.persist_directory, ignore_errors=True)\n",
    "    \n",
    "    # Create a new vectorstore\n",
    "    logger.info(\"Creating new vectorstore...\")\n",
    "    if not self.documents:\n",
    "        self.load_documents()\n",
    "    \n",
    "    # Create a temporary directory for the database\n",
    "    # This helps avoid permission issues on some systems\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    logger.info(f\"Using temporary directory for initial database creation: {temp_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # First create in temp directory\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=temp_dir\n",
    "        )\n",
    "        \n",
    "        # Now create the real directory\n",
    "        if not os.path.exists(self.persist_directory):\n",
    "            os.makedirs(self.persist_directory)\n",
    "            \n",
    "        # And create the final vectorstore\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.persist_directory\n",
    "        )\n",
    "        self.vectorstore.persist()\n",
    "        \n",
    "        logger.info(f\"Vectorstore created successfully with {len(self.documents)} documents\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating vectorstore: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up temp directory\n",
    "        if os.path.exists(temp_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0353788",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2d333b; padding: 5px; border-radius: 4px; margin-bottom: 10px;\">\n",
    "  <h3 style=\"color: #58a6ff; margin: 10px;\">3.3 Building the RAG Chain</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 15px; border-radius: 8px; border-left: 4px solid #79c0ff; margin-bottom: 20px;\">\n",
    "  <p style=\"color: #adbac7;\">Here's where our system turns into a \"detective.\" We connect all the components into a sequence that transforms questions into accurate answers.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #1c2128; padding: 20px; border-radius: 8px; margin: 20px 0; border: 1px solid #444c56;\">\n",
    "  <h4 style=\"color: #79c0ff; margin-top: 0; text-align: center; margin-bottom: 20px;\">The RAG Chain Components</h4>\n",
    "  \n",
    "  <!-- Retriever Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 15px; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">üîç</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The Searcher (Retriever)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Turns your question into a fingerprint and finds the closest matches in the database.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Prompt Template Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 15px; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">üìù</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The Instructions (Prompt)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Like a recipe: \"Be nice, use the chunks, cite your sources.\" This keeps answers helpful and trustworthy.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">prompt = PromptTemplate.from_template(template)</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <!-- LLM Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 15px; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">üß†</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The AI (LLM)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Writes the final response based on the instructions and retrieved chunks.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">llm = ChatOllama(model=\"llama3\", temperature=0.1)</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Output Parser Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 0; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">‚ú®</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The Formatter (Parser)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Makes the response neat and clear for the user to read.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">StrOutputParser()</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<!-- Flow diagram -->\n",
    "<div style=\"background-color: #22272e; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h4 style=\"color: #79c0ff; margin-top: 0; text-align: center;\">How It All Flows Together</h4>\n",
    "  \n",
    "  <div style=\"display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin: 20px 0;\">\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">‚ùì</div>\n",
    "      <div style=\"color: #adbac7;\">Question</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üîç</div>\n",
    "      <div style=\"color: #adbac7;\">Retriever</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üìù</div>\n",
    "      <div style=\"color: #adbac7;\">Prompt</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üß†</div>\n",
    "      <div style=\"color: #adbac7;\">LLM</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">‚ú®</div>\n",
    "      <div style=\"color: #adbac7;\">Parser</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üí°</div>\n",
    "      <div style=\"color: #adbac7;\">Answer</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"background-color: #1c2128; padding: 15px; border-radius: 8px; margin-top: 20px;\">\n",
    "    <p style=\"color: #adbac7; margin: 0; text-align: center;\">This entire chain is created with just a few lines of code:</p>\n",
    "    <div style=\"background-color: #2d333b; border-radius: 5px; padding: 15px; margin-top: 10px; font-family: monospace;\">\n",
    "      <pre style=\"color: #d2a8ff; margin: 0; overflow-x: auto; font-size: 0.9em;\">self.chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | self.llm\n",
    "    | StrOutputParser()\n",
    ")</pre>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border-left: 4px solid #58a6ff; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #adbac7; margin: 0;\"><strong style=\"color: #adbac7;\">üí° Pro Tip:</strong> The key to a good RAG system is balance. A great prompt template with poor retrieval won't work well, and perfect retrieval with bad instructions will still give bad answers. All pieces need to work together!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chain(self) -> None:\n",
    "    \"\"\"Set up the RAG chain for question answering\"\"\"\n",
    "    if not self.vectorstore:\n",
    "        self.create_vectorstore()\n",
    "    \n",
    "    # Create retriever with search parameters\n",
    "    retriever = self.vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}  # Return top 3 most relevant chunks\n",
    "    )\n",
    "    \n",
    "    # Define the prompt template\n",
    "    template = \"\"\"\n",
    "### INSTRUCTIONS:\n",
    "    You are an expert in robotics and wearable robots. You are going to help me write a state of the art article based on the documents provided, which are focused on the use of physical dummies for the study or improvement of physical human-exoskeleton interaction. These dummies, or surrogates, or mannequins, are robotic replicas of humans or parts of humans. Base your answers strictly on the documents provided. \n",
    "    Be polite, professional, and avoid guessing or using outside sources.\n",
    " \n",
    "    (1) Be attentive to details: read the question and the context thoroughly before answering.\n",
    "    (2) Begin your response with a friendly tone and briefly restate the users question to confirm understanding.\n",
    "    (3) If the context allows you to answer the question, write a detailed, helpful, and easy-to-understand response.\n",
    "        - Use precise terminology from the articles \n",
    "        - Reference the sources **inline** (e.g., [Article 1 ¬ß3.2], [Article 2 Fig.4]) and ONLY cite sections/figures/tables present in the provided context.\n",
    "      IF NOT: if you cannot find the answer, respond with an explanation, starting with: \n",
    "        \"I couldn't find the information in the documents I have access to.\"\n",
    "    (4) Below your response, list all referenced sources (document titles/IDs and exact sections/figures/tables that support your claims).\n",
    "    (5) Review your answer to ensure it answers the question, is helpful and professional, and is formatted for easy reading (short paragraphs, bullets if useful).\n",
    "    Additional constraints:\n",
    "    - Do not invent citations or content outside the provided context.\n",
    "    - If there are conflicting statements in the articles, acknowledge the discrepancy and cite them.\n",
    " \n",
    "    THINK STEP BY STEP\n",
    " \n",
    "    Answer the following question using the provided context.\n",
    "    ### Question: {question} ###\n",
    "    ### Context: {context} ###\n",
    "    ### Helpful Answer with Sources:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    # Create the chain\n",
    "    self.chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | self.llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    logger.info(\"RAG chain setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(self, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using the RAG chain\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        \n",
    "    Returns:\n",
    "        The answer to the question\n",
    "    \"\"\"\n",
    "    if not self.chain:\n",
    "        self.setup_chain()\n",
    "    \n",
    "    logger.info(f\"Answering question: {question}\")\n",
    "    try:\n",
    "        answer = self.chain.invoke(question)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error answering question: {e}\")\n",
    "        return f\"Error processing your question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c35852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface(rag_system: RAGSystem) -> gr.Interface:\n",
    "    \"\"\"Create an enhanced Gradio interface for the RAG system\"\"\"\n",
    "    \n",
    "    # Custom CSS for better styling\n",
    "    custom_css = \"\"\"\n",
    "    .container {\n",
    "        max-width: 900px;\n",
    "        margin: auto;\n",
    "    }\n",
    "    .gr-button-primary {\n",
    "        background: linear-gradient(45deg, #3b82f6, #0ea5e9) !important;\n",
    "        border: none !important;\n",
    "    }\n",
    "    .gr-button-primary:hover {\n",
    "        background: linear-gradient(45deg, #2563eb, #0284c7) !important;\n",
    "    }\n",
    "    .title-text {\n",
    "        text-align: center;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "    .example-text {\n",
    "        font-size: 0.9em;\n",
    "        font-style: italic;\n",
    "    }\n",
    "    .markdown-text {\n",
    "        font-size: 1.1em;\n",
    "        line-height: 1.5;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def process_question(question: str, history: list) -> tuple[list, str]:\n",
    "        \"\"\"Enhanced question processing with chat history\"\"\"\n",
    "        if not question.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        try:\n",
    "            # Get the answer\n",
    "            answer = rag_system.answer_question(question)\n",
    "            # Apply cleaning using RAGSystem's static method\n",
    "            answer = RAGSystem.clean_text(answer)\n",
    "            \n",
    "            history = history + [(question, answer)]\n",
    "            return history, \"\"  # Clear input after submission\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing question: {e}\")\n",
    "            return history + [(question, f\"Error: {str(e)}\")], \"\"\n",
    "\n",
    "    with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as interface:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # ü§ñ Physical Human-Exoskeleton Interaction Research Assistant\n",
    "            \n",
    "            Ask questions about physical dummies, human-exoskeleton interaction, testing methods, and related topics.\n",
    "            All responses are based on academic papers with proper citations.\n",
    "            \"\"\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Research Discussion\",\n",
    "            height=400,\n",
    "            show_copy_button=True\n",
    "        )\n",
    "        \n",
    "        question = gr.Textbox(\n",
    "            placeholder=\"Ask about physical dummies, testing methods, design approaches...\",\n",
    "            label=\"Research Question\",\n",
    "            lines=3\n",
    "        )\n",
    "        \n",
    "        submit = gr.Button(\"Submit Question\", variant=\"primary\")\n",
    "        clear = gr.Button(\"Clear History\")\n",
    "\n",
    "        with gr.Accordion(\"Example Questions\", open=False):\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    \"What are the main applications of physical dummies in exoskeleton testing?\",\n",
    "                    \"How are physical surrogates designed for human-robot interaction?\",\n",
    "                    \"What measurement methods are used with physical dummies?\",\n",
    "                    \"What are the advantages of using dummies over human subjects?\",\n",
    "                    \"Which sensors are commonly used in physical dummy testing?\"\n",
    "                ],\n",
    "                inputs=question,\n",
    "                label=\"Click an example to try it\"\n",
    "            )\n",
    "\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### About This System\n",
    "            \n",
    "            - Specialized in physical dummies, or mannequins, and human-exoskeleton interaction research\n",
    "            - Provides detailed answers with proper citations from academic papers\n",
    "            - Uses precise terminology from source documents\n",
    "            - Covers topics like design approaches, testing methods, and evaluation techniques\n",
    "            \n",
    "            *Note: Responses are limited to information present in the loaded research papers.*\n",
    "            \"\"\")\n",
    "\n",
    "        # Set up event handlers\n",
    "        submit.click(\n",
    "            fn=process_question,\n",
    "            inputs=[question, chatbot],\n",
    "            outputs=[chatbot, question],\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        # Handle question submission with Enter key\n",
    "        question.submit(\n",
    "            fn=process_question,\n",
    "            inputs=[question, chatbot],\n",
    "            outputs=[chatbot, question],\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        # Clear chat history\n",
    "        clear.click(\n",
    "            lambda: ([], \"\"),\n",
    "            outputs=[chatbot, question],\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dfbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main function to run the RAG system and launch the interface\"\"\"\n",
    "    try:\n",
    "        # Display banner\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üåä DUMMY state of the art\")\n",
    "        print(\"   Review paper Document Intelligence Assistant\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display available models\n",
    "        print(\"\\n==== CHECKING OLLAMA MODELS ====\")\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "            print(\"Available Ollama models:\")\n",
    "            if response.status_code == 200:\n",
    "                models = response.json().get(\"models\", [])\n",
    "                if models:\n",
    "                    for model in models:\n",
    "                        print(f\"‚úì {model['name']}\")\n",
    "                else:\n",
    "                    print(\"‚ùå No models found\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error checking Ollama models: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "            print(\"   Make sure Ollama is running with: ollama serve\")\n",
    "        \n",
    "        print(f\"\\nüìã CONFIGURATION:\")\n",
    "        print(f\"   LLM model: {LLM_MODEL}\")\n",
    "        print(f\"   Embedding model: {EMBEDDING_MODEL}\")\n",
    "        print(f\"   Documents to process: {len(PDF_URLS)} thesis articles\")\n",
    "        print(\"\\n   Make sure these models are available with 'ollama pull' commands.\")\n",
    "        \n",
    "        # Create and initialize the RAG system\n",
    "        print(\"\\n==== INITIALIZING RAG SYSTEM ====\")\n",
    "        logger.info(\"Creating RAG system...\")\n",
    "        rag_system = RAGSystem(pdf_urls=PDF_URLS)\n",
    "        \n",
    "        # Load documents and create vectorstore\n",
    "        print(\"üìö Loading documents...\")\n",
    "        rag_system.load_documents()\n",
    "        \n",
    "        print(\"üîç Creating vector embeddings...\")\n",
    "        rag_system.create_vectorstore()\n",
    "        \n",
    "        # Test with a control question about hydrology/drought analysis\n",
    "        print(\"\\n==== TESTING SYSTEM ====\")\n",
    "        logger.info(\"Testing with a control question...\")\n",
    "        test_questions = [\n",
    "            \"What is a physical dummy?\",\n",
    "            \"Which are their advantages for use in the study of human-exoskeleton interaction?\",\n",
    "        ]\n",
    "        \n",
    "        # Try the first available test question\n",
    "        test_question = test_questions[0]\n",
    "        print(f\"üß™ Testing with: '{test_question}'\")\n",
    "        test_answer = rag_system.answer_question(test_question)\n",
    "        \n",
    "        if test_answer and len(test_answer) > 50:\n",
    "            logger.info(f\"‚úì Control answer received (length: {len(test_answer)})\")\n",
    "            print(\"‚úì System test successful - RAG pipeline working correctly\")\n",
    "        else:\n",
    "            logger.warning(f\"‚ö†Ô∏è Short control answer received (length: {len(test_answer)})\")\n",
    "            print(\"‚ö†Ô∏è System test completed but response seems short\")\n",
    "        \n",
    "        # Create and launch Gradio interface\n",
    "        print(\"\\n==== LAUNCHING INTERFACE ====\")\n",
    "        logger.info(\"Launching Gradio interface...\")\n",
    "        print(\"üöÄ Starting web interface...\")\n",
    "        print(\"   - Access locally at: http://localhost:7860\")\n",
    "        print(\"   - Interface optimized for analysis of the state of the art on physical dummies\")\n",
    "        print(\"   - All responses based on your documents\")\n",
    "        \n",
    "        # Use our custom interface\n",
    "        interface = create_gradio_interface(rag_system)\n",
    "        interface.launch(\n",
    "            share=False,  # Set share=True to create a public link\n",
    "            inbrowser=True,  # Automatically open browser\n",
    "            show_error=True,\n",
    "            quiet=False\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main function: {e}\")\n",
    "        print(f\"\\n\\n‚ùå ERROR: {str(e)}\\n\\n\")\n",
    "        print(\"üîß TROUBLESHOOTING TIPS FOR RAG SYSTEM:\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1. üñ•Ô∏è  OLLAMA SERVICE:\")\n",
    "        print(\"   - Make sure Ollama is running: 'ollama serve'\")\n",
    "        print(\"   - Check if Ollama is accessible at http://localhost:11434\")\n",
    "        print()\n",
    "        print(\"2. üß† REQUIRED MODELS:\")\n",
    "        print(f\"   - Pull LLM model: 'ollama pull {LLM_MODEL}'\")\n",
    "        print(f\"   - Pull embedding model: 'ollama pull {EMBEDDING_MODEL}'\")\n",
    "        print(\"   - For hydrology, recommend: llama3.1:8b or mistral:7b\")\n",
    "        print()\n",
    "        print(\"3. üìÑ DOCUMENT PROCESSING:\")\n",
    "        print(\"   - Verify PDF URLs are accessible\")\n",
    "        print(\"   - Check thesis documents are properly formatted\")\n",
    "        print(\"   - Ensure PDFs contain extractable text\")\n",
    "        print()\n",
    "        print(\"4. üîß TECHNICAL ISSUES:\")\n",
    "        print(\"   - If dimension mismatch: try 'nomic-embed-text' embedding model\")\n",
    "        print(\"   - Check Python packages: pip install -r requirements.txt\")\n",
    "        print(\"   - Verify Chroma vector database permissions\")\n",
    "        print()\n",
    "        print(\"5. üìä PERFORMANCE OPTIMIZATION:\")\n",
    "        print(\"   - For better hydrology responses, use larger models (13B+)\")\n",
    "        print(\"   - Increase chunk_size for technical documents\")\n",
    "        print(\"   - Adjust temperature for more/less conservative answers\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for method assignments since they are now part of the class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the system\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    # If running in a notebook\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
